<#

.SYNOPSIS
  Backup files in an AWS S3 bucket for Point in Time Restore.
  Works with files uploaded using the AWS CLI using "aws s3 sync"
  This will copy your files into S3 and write some metadata files recording:
   - Original Creation Date
   - Original Last Write
   - Windows NTFS DACLS (permissions) of files and folders
  
.DESCRIPTION
  The purpose of this is to copy files from the hard drive into S3 in a certain format.
  This format should allow you to browse the files in the S3 Browser (web) by AWS 
  This also records meta data. Later on I plan to add file line counts to generate differential reports quickly.
  
  Lets say I want to backup a folder: c:\Users\gordon\Desktop
  
  After I have AWS CLI and an S3 bucket (november2022demo) setup, I would use the AWS CLi to back it up like this:
  
  - aws s3 sync C:\Users\Gordon\desktop s3://november2022demo/files/ --delete
  
  
  Now, if I wanted to restore the most recent backup, I'd swap the source and destination:
  - aws s3 sync s3://november2022demo/files/ C:\Users\Gordon\desktop --delete
  
  
  But, lets say I was hit with a ransomware and all my files were encrypted or deleted. And... my backup has copied all the encrypted files to s3.
  How to I restore from a "point in time?" This is professionally known as PITR, point in time recovery.
  This script looks at a subfolder in a bucket and reads the AWS metadata of all file.  

  Since S3 is more like an SQL entry, not a file system, but uses a file system convention for access, you manually have to save metadata on permissions and empty folders. (s3 will not copy empty folders.)
  The meta data is generated by the aws cli, icacls, and powershell.
  
.PARAMETER 
  4 Variables at the top of the script
  
.INPUTS
  This pushes data with the AWS CLI into an S3 Bucket
  This requires you to set up the AWS CLI, your account, a user account, connection keys, an S3 Bucket (version enabled), and the IAM Permissions for S3 access > Role > Add said role to user.
  This requires you to type in "aws configure" in the command line and link your access key/secret to your AWS CLI.
  
.OUTPUTS
  The output is getting your data into S3
  
.NOTES
  Version:        1.0
  Author:         /u/gordonv
  Creation Date:  12/09/2022
  Purpose/Change: Writing a restore function for Windows Files from S3.
  Site:           https://github.com/GordonVi/win2s3/
  
.EXAMPLE
Run the PS1 script. It doesn't request prompts. It can be scheduled.
#>

# --------------------------------

$target = "C:\Users\gordon\Desktop"
$bucket_prefix="november2022demo"
$bucketname = "$bucket_prefix".ToLower()
$local_temp_folder = "c:\temp\win2s3"

# --------------------------------

$target_bucket_subfolder_name = $target.replace("\","/").replace(":","")


# Write Meta Files in local temp folder
aws s3 sync "$target" "s3://$bucketname/files/$target_bucket_subfolder_name/" --delete --dryrun > "$local_temp_folder\aws_s3_dryrun.txt"
icacls "$target" /save "$local_temp_folder\icacls.txt" /t /c

# generate a list of files and folders from powershell
# this is how we know what empty directories exist and to recreate.
# --------------------------------

	$list = $(gci $target -recurse) | select name, PSIsContainer, length, CreationTimeUTC, LastWriteTimeUTC, isreadonly, directoryname, fullname

# ---

	# Add Recursive file count per folder
		$combined_list = foreach ($item in $list) {

		$OutputItem = $item
		
		if ($item.PSIsContainer -eq 1) {
			
			$OutputItem | Add-Member -NotePropertyName "recurse_file_count" $(gci $item.fullname -recurse | where PSIsContainer -eq 0).count
	
		}

		$OutputItem
	}

# ---

	$combined_list | convertto-json | out-file "$local_temp_folder\file_list.powershell.json"

# --------------------------------


aws s3 sync "$target" "s3://$bucketname/files/$target_bucket_subfolder_name/" --delete

aws s3api list-objects-v2 --bucket $bucketname --prefix "files/$target_bucket_subfolder_name" --output json > "$local_temp_folder\s3api_file_list.json"
aws s3 sync "$local_temp_folder" "s3://$bucketname/metadata/$target_bucket_subfolder_name/" --delete

